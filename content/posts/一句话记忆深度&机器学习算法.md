---
title: "一句话记忆深度&机器学习算法"
date: 2020-04-20T08:09:51+08:00
tags:
    - machine learning
    - deep learning
    - 收集
draft: true
---

## 说明
尽量试试用很少的语言去总结机器学习和深度学习算法。事实上这个是很难做的，其实是给自己加深某些算法的关键记忆点。

## 通常机器学习
这里是普通机器学习方法的总结。

### XGBoost
Boosting方法，通过加新树的不断提升方法来提高准确率。主要创新在其公式的变化让其能更简单的学习增益裂变值，其优势速度快，其学习方式使得树的生成更准确。但是一些细节也让XGBoost更好更快：缺失值的处理，学习缺失值的默认方向；L1、L2正则化；列采样等一些防止过拟合方法；学习率帮助收敛；支持分布式。在很多实操中，一般都是注意这几个参数：树规模，树深度，采样参数，正则化参数，学习率。调好他们一般都能取得很好的结果。

### LSH, Locality sensitive Hash
局部敏感哈希。从多维空间上说就是给点赋hashcode，距离相同的点基于hashcode的海明距离较小，即相似。所以在多维空间建立分割超平面来给点分配hashcode，这个过程分割的方法就很重要了。迭代法啊这类的。

## 深度学习
其实搞了2年左右的深度学习，感觉最近论坛对AI落地这里的确悲观。后来跑去写接口去了。

### 卷积神经网络

#### unet
我使用最多的神经网络，一般用在医疗图像。u型结构，左右对应桥接，有深度，深度学习物体类别特征，再向左借高分辨率物体轮廓信息学习形态，这样就可以学到物体的切割实例。

### 时序神经网络

#### attention机制，transforms
其实名字从文章attention is all you need中可知，准确说是self-attention，一般用在翻译上。一句话的前后单词之间的语义关系可以自己和自己通过权重乘积来学习，然后用softmax来分化强化这种联系。自己查自己的值，用通常q，k，v来计算：query到key来找value。深度学习是玩权重的，遇事不决套权重计算，q k v自然用一些网络层来计算。这里有个教你写代码[TRANSFORMERS FROM SCRATCH](http://www.peterbloem.nl/blog/transformers)，感觉很好。

### 图卷积神经网络

#### GCN

GCN出来很关注，因为我以前就是搞图挖掘的，虽然很水就是了。神经网络就是玩权重的，做卷积的。现在就是在图结构上做卷积学习图的特征。其公式一般就是

$f(H^{l},A)=\alpha（D^{-\frac{1}{2}}\hat{A}D^{-\frac{1}{2}}H^{l}W^{l}）$

其中$\hat{A}=A+I$, $A$是邻接矩阵，$I$是单位矩阵，即节点自己和自己连接矩阵，加起来就是完全的连接信息，直接点就是图的一种表示。$D$是节点度矩阵，图表示归一化就是$D^{-1}A$，但是他不是对称的，所以$D^{-\frac{1}{2}}\hat{A}D^{-\frac{1}{2}}$ 拉普拉斯归一化方法使得归一化结果是对称的，保持图结构信息不变。$H^{l}$和$W^{l}$ 就是输入节点特征和权重了，做卷积。一句话就是在图特征上做卷积计算，学习权重。其实算法目标也是图节点聚类，所以算法为了训练变成半监督式的算法。

当然没有这么简单，其实来源是ChebNet一步步变化出来的。这里就不多说了。具体可以看这个文章：[图神经网络 GNN GAT & GCN - 1](https://zhuanlan.zhihu.com/p/133282394)

拉普拉斯归一化的应用可以看这篇论文，Fast Random Walk with Restart and Its Applications。

### Bayesian Deep Learning
贝叶斯深度学习方法就是将以前学习明确的权重变为学习这些权重的分布，得到一个不明确的预测值范围。其中的一些方法对有限数据集有很好的效果。下面是一些有趣的文章算法。

#### Dropout as a Baysian Approximation
正常训练网络，然后使用Monte Carlo Dropout方法随机放弃某些神经元（某层的某些卷积结果）来预测结果，这样得出的很多个结果呈现成一种分布，类似正态分布。

#### Neural Processes
学习y=f(x)的分布，也就是x,y关系的分布。分成两个部分，一个学习x，y的分布，一个通过这个分布再和其他未知的x一起来预测其对应的y的分布。即一个是学习隐式分布网络，一个是预测结果网络，当然结果是一个分布区间。

### ODE(ordinary differential equations) network
基本思路是残差网络的形式和常微分方程很像，那么常规神经网络的隐藏层其实是可以使用ODE solvers来代替，隐藏层的每次阶梯性计算变为ODE solvers的时序性变化。这样会减少参数和内存或者显存消耗。简单理解这里：[论文作者的解释](https://news.ycombinator.com/item?id=18676986)。具体的参数更新方法论文应该更详细[nips paper ode net](https://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf)

### normalize flow
标准化流就是设计双射函数将当前分布转换成另一种分布。[evjiang 博客](https://blog.evjang.com/2018/01/nf1.html)

### 数据增强和训练方式
关于数据增强，其实对于深度学习和机器学习来说都是想办法增加样本数和保持样本平衡。其实对于目标检测最好也要保持卷积网络对目标物体在整个空间的感受。

训练方式其实借鉴fastai库里一些集成的方法就很好。

<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>